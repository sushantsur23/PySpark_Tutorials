{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "108b04b8-232a-4535-9b02-baec61486013",
   "metadata": {},
   "source": [
    "PySpark Collect() â€“ Retrieve data from DataFrame\n",
    "\n",
    "PySpark RDD/DataFrame collect() is an action operation that is used to retrieve all the elements of the dataset (from all nodes) to the driver node. We should use the collect() on smaller dataset usually after filter(), group() e.t.c. Retrieving larger datasets results in OutOfMemory error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45f99290-a403-47f2-a2e3-af9b4893a6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebfb1cf4-f38d-4497-acec-853d72580510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(dept_name='Finance', dept_id=10), Row(dept_name='Marketing', dept_id=20), Row(dept_name='Sales', dept_id=30), Row(dept_name='IT', dept_id=40)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataCollect = deptDF.collect()\n",
    "print(dataCollect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66a7cb3c-aec1-49fc-8d31-c9988d279542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dept_name='Finance', dept_id=10),\n",
       " Row(dept_name='Marketing', dept_id=20),\n",
       " Row(dept_name='Sales', dept_id=30),\n",
       " Row(dept_name='IT', dept_id=40)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "[Row(dept_name='Finance', dept_id=10), \n",
    "Row(dept_name='Marketing', dept_id=20), \n",
    "Row(dept_name='Sales', dept_id=30), \n",
    "Row(dept_name='IT', dept_id=40)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95eb64e1-d6f7-49a9-a8b0-bec34128cf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finance,10\n",
      "Marketing,20\n",
      "Sales,30\n",
      "IT,40\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for row in dataCollect:\n",
    "    print(row['dept_name'] + \",\" +str(row['dept_id']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce78c76a-26e4-48ca-bd12-743f40063b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finance,10\n",
      "Marketing,20\n",
      "Sales,30\n",
      "IT,40\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for row in dataCollect:\n",
    "    print(row['dept_name'] + \",\" +str(row['dept_id']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fd101a5-728d-47c0-a6aa-60a415857822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Finance'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Returns value of First Row, First Column which is \"Finance\"\n",
    "deptDF.collect()[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adfd8e93-8781-4a1e-9daa-0d1c800e81a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataCollect = deptDF.select(\"dept_name\").collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df3716d-ee3f-414f-a1ef-c7362a3a5d11",
   "metadata": {},
   "source": [
    "When to avoid Collect()\n",
    "Usually, collect() is used to retrieve the action output when you have very small result set and calling collect() on an RDD/DataFrame with a bigger result set causes out of memory as it returns the entire dataset (from all workers) to the driver hence we should avoid calling collect() on a larger dataset.\n",
    "\n",
    "collect () vs select ()\n",
    "select() is a transformation that returns a new DataFrame and holds the columns that are selected whereas collect() is an action that returns the entire data set in an Array to the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15a7fa6-72b3-4cde-9779-0c3c63e70838",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
