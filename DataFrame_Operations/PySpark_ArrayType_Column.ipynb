{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45425e5b-6e0d-48bc-af8d-87347fc29ad0",
   "metadata": {},
   "source": [
    "PySpark ArrayType Column\n",
    "\n",
    "PySpark pyspark.sql.types.ArrayType (ArrayType extends DataType class) is used to define an array data type column on DataFrame that holds the same type of elements, In this article, I will explain how to create a DataFrame ArrayType column using org.apache.spark.sql.types.ArrayType class and applying some SQL functions on the array columns with examples.\n",
    "\n",
    "While working with structured files (Avro, Parquet e.t.c) or semi-structured (JSON) files, we often get data with complex structures like MapType, ArrayType, StructType e.t.c. I will try my best to cover some mostly used functions on ArrayType columns.\n",
    "\n",
    "What is PySpark ArrayType\n",
    "PySpark ArrayType is a collection data type that extends the DataType class which is a superclass of all types in PySpark. All elements of ArrayType should have the same type of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24ada7e1-8b95-487f-b795-d4294fb40ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "arrayCol = ArrayType(StringType(),False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b67b14a8-fe03-4299-b8d6-1110a69118cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- languagesAtWork: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- currentState: string (nullable = true)\n",
      " |-- previousState: string (nullable = true)\n",
      "\n",
      "+----------------+------------------+---------------+------------+-------------+\n",
      "|            name| languagesAtSchool|languagesAtWork|currentState|previousState|\n",
      "+----------------+------------------+---------------+------------+-------------+\n",
      "|    James,,Smith|[Java, Scala, C++]|  [Spark, Java]|          OH|           CA|\n",
      "|   Michael,Rose,|[Spark, Java, C++]|  [Spark, Java]|          NY|           NJ|\n",
      "|Robert,,Williams|      [CSharp, VB]|[Spark, Python]|          UT|           NV|\n",
      "+----------------+------------------+---------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"SparkByExamples.com\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"SparkByExamples.com\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = [\n",
    " (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"],\"OH\",\"CA\"),\n",
    " (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"],\"NY\",\"NJ\"),\n",
    " (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"],\"UT\",\"NV\")\n",
    "]\n",
    "\n",
    "from pyspark.sql.types import StringType, ArrayType,StructType,StructField\n",
    "schema = StructType([ \n",
    "    StructField(\"name\",StringType(),True), \n",
    "    StructField(\"languagesAtSchool\",ArrayType(StringType()),True), \n",
    "    StructField(\"languagesAtWork\",ArrayType(StringType()),True), \n",
    "    StructField(\"currentState\", StringType(), True), \n",
    "    StructField(\"previousState\", StringType(), True)\n",
    "  ])\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "df.printSchema()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2b8b497-f5f9-4470-8a9d-ab5530c4227c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+\n",
      "|            name|   col|\n",
      "+----------------+------+\n",
      "|    James,,Smith|  Java|\n",
      "|    James,,Smith| Scala|\n",
      "|    James,,Smith|   C++|\n",
      "|   Michael,Rose,| Spark|\n",
      "|   Michael,Rose,|  Java|\n",
      "|   Michael,Rose,|   C++|\n",
      "|Robert,,Williams|CSharp|\n",
      "|Robert,,Williams|    VB|\n",
      "+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Use explode() function to create a new row for each element in the given array column.\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "df.select(df.name,explode(df.languagesAtSchool)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70ed22ed-7853-4924-8847-4564ddda193f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         nameAsArray|\n",
      "+--------------------+\n",
      "|    [James, , Smith]|\n",
      "|   [Michael, Rose, ]|\n",
      "|[Robert, , Williams]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#split() sql function returns an array type after splitting the string column by delimiter. Below example split the name column by comma delimiter.\n",
    "\n",
    "from pyspark.sql.functions import split\n",
    "df.select(split(df.name,\",\").alias(\"nameAsArray\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dab28c1-d046-49d3-b812-9436de01ac97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+\n",
      "|            name|  States|\n",
      "+----------------+--------+\n",
      "|    James,,Smith|[OH, CA]|\n",
      "|   Michael,Rose,|[NY, NJ]|\n",
      "|Robert,,Williams|[UT, NV]|\n",
      "+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Use array() function to create a new array column by merging the data from multiple columns. All input columns must have the same data type. The below example combines the data from currentState and previousState and creates a new column states.\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import array\n",
    "df.select(df.name,array(df.currentState,df.previousState).alias(\"States\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b58268f-06b7-4cc3-9725-f4b0b7a3c019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+\n",
      "|            name|array_contains|\n",
      "+----------------+--------------+\n",
      "|    James,,Smith|          true|\n",
      "|   Michael,Rose,|          true|\n",
      "|Robert,,Williams|         false|\n",
      "+----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#array_contains() sql function is used to check if array column contains a value. Returns null if the array is null, true if the array contains the value, and false otherwise\n",
    "\n",
    "from pyspark.sql.functions import array_contains\n",
    "df.select(df.name,array_contains(df.languagesAtSchool,\"Java\")\n",
    "    .alias(\"array_contains\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010e72ec-a496-4c13-8b1b-0f9d2103e0e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
