{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec348508-469e-4d83-8d8c-3122405353d2",
   "metadata": {},
   "source": [
    "PySpark Row using on DataFrame and RDD\n",
    "\n",
    "In PySpark Row class is available by importing pyspark.sql.Row which is represented as a record/row in DataFrame, one can create a Row object by using named arguments, or create a custom Row like class.\n",
    "\n",
    "Key Points of Row Class:\n",
    "\n",
    "Earlier to Spark 3.0, when used Row class with named arguments, the fields are sorted by name.\n",
    "\n",
    "Since 3.0, Rows created from named arguments are not sorted alphabetically instead they will be ordered in the position entered.\n",
    "\n",
    "To enable sorting by names, set the environment variable PYSPARK_ROW_FIELD_SORTING_ENABLED to true.\n",
    "\n",
    "Row class provides a way to create a struct-type column as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60aaa090-2957-420e-9a3f-a4b80ac35f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James,40\n"
     ]
    }
   ],
   "source": [
    "#Create a Row Object\n",
    "\n",
    "\n",
    "from pyspark.sql import Row\n",
    "row=Row(\"James\",40)\n",
    "print(row[0] +\",\"+str(row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e5aa152-8b2b-4abe-8feb-e9de413b3e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice\n"
     ]
    }
   ],
   "source": [
    "#Alternatively\n",
    "row=Row(name=\"Alice\", age=11)\n",
    "print(row.name) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "345cfd09-7c7f-42aa-b969-d8a15f0fcdb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James,Alice\n"
     ]
    }
   ],
   "source": [
    "#Create Custom Class from Row\n",
    "\n",
    "\n",
    "Person = Row(\"name\", \"age\")\n",
    "p1=Person(\"James\", 40)\n",
    "p2=Person(\"Alice\", 35)\n",
    "print(p1.name +\",\"+p2.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10089715-7cc8-4566-b0b4-a8cdb3a9f116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(name='James,,Smith', lang=['Java', 'Scala', 'C++'], state='CA'), Row(name='Michael,Rose,', lang=['Spark', 'Java', 'C++'], state='NJ'), Row(name='Robert,,Williams', lang=['CSharp', 'VB'], state='NV')]\n"
     ]
    }
   ],
   "source": [
    "#Using Row class on PySpark RDD\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "data = [Row(name=\"James,,Smith\",lang=[\"Java\",\"Scala\",\"C++\"],state=\"CA\"), \n",
    "    Row(name=\"Michael,Rose,\",lang=[\"Spark\",\"Java\",\"C++\"],state=\"NJ\"),\n",
    "    Row(name=\"Robert,,Williams\",lang=[\"CSharp\",\"VB\"],state=\"NV\")]\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "print(rdd.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ed1ec23-2572-407d-862e-d2175b6d6214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James,,Smith,['Java', 'Scala', 'C++']\n",
      "Michael,Rose,,['Spark', 'Java', 'C++']\n",
      "Robert,,Williams,['CSharp', 'VB']\n"
     ]
    }
   ],
   "source": [
    "#Now, let’s collect the data and access the data using its properties.\n",
    "\n",
    "\n",
    "collData=rdd.collect()\n",
    "for row in collData:\n",
    "    print(row.name + \",\" +str(row.lang))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67de2b88-ed48-497a-86ab-df92da696b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternatively, you can also do by creating a Row like class “Person”\n",
    "\n",
    "Person=Row(\"name\",\"lang\",\"state\")\n",
    "data = [Person(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \n",
    "    Person(\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"),\n",
    "    Person(\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f88c1ad-3984-40f8-8340-6574be39c1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- lang: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n",
      "+----------------+------------------+-----+\n",
      "|            name|              lang|state|\n",
      "+----------------+------------------+-----+\n",
      "|    James,,Smith|[Java, Scala, C++]|   CA|\n",
      "|   Michael,Rose,|[Spark, Java, C++]|   NJ|\n",
      "|Robert,,Williams|      [CSharp, VB]|   NV|\n",
      "+----------------+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using Row class on PySpark DataFrame\n",
    "\n",
    "\n",
    "df=spark.createDataFrame(data)\n",
    "df.printSchema()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa498de2-a260-4b9a-9061-dc8e2d8eec95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- currentState: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#You can also change the column names by using toDF() function\n",
    "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
    "df=spark.createDataFrame(data).toDF(*columns)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed75aa2e-9e72-4a26-ac78-d069da899f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- prop: struct (nullable = true)\n",
      " |    |-- hair: string (nullable = true)\n",
      " |    |-- eye: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Nested Struct Using Row Class\n",
    "\n",
    "#Create DataFrame with struct using Row class\n",
    "from pyspark.sql import Row\n",
    "data=[Row(name=\"James\",prop=Row(hair=\"black\",eye=\"blue\")),\n",
    "      Row(name=\"Ann\",prop=Row(hair=\"grey\",eye=\"black\"))]\n",
    "df=spark.createDataFrame(data)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3adfcf2-de04-4f8a-985c-a10f22b34833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James,40\n",
      "Alice\n",
      "James,Alice\n",
      "[Row(name='James,,Smith', lang=['Java', 'Scala', 'C++'], state='CA'), Row(name='Michael,Rose,', lang=['Spark', 'Java', 'C++'], state='NJ'), Row(name='Robert,,Williams', lang=['CSharp', 'VB'], state='NV')]\n",
      "James,,Smith,['Java', 'Scala', 'C++']\n",
      "Michael,Rose,,['Spark', 'Java', 'C++']\n",
      "Robert,,Williams,['CSharp', 'VB']\n",
      "[Row(name='James,,Smith', lang=['Java', 'Scala', 'C++'], state='CA'), Row(name='Michael,Rose,', lang=['Spark', 'Java', 'C++'], state='NJ'), Row(name='Robert,,Williams', lang=['CSharp', 'VB'], state='NV')]\n",
      "James,,Smith,['Java', 'Scala', 'C++']\n",
      "Michael,Rose,,['Spark', 'Java', 'C++']\n",
      "Robert,,Williams,['CSharp', 'VB']\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- lang: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n",
      "+----------------+------------------+-----+\n",
      "|            name|              lang|state|\n",
      "+----------------+------------------+-----+\n",
      "|    James,,Smith|[Java, Scala, C++]|   CA|\n",
      "|   Michael,Rose,|[Spark, Java, C++]|   NJ|\n",
      "|Robert,,Williams|      [CSharp, VB]|   NV|\n",
      "+----------------+------------------+-----+\n",
      "\n",
      "[Row(name='James,,Smith', lang=['Java', 'Scala', 'C++'], state='CA'), Row(name='Michael,Rose,', lang=['Spark', 'Java', 'C++'], state='NJ'), Row(name='Robert,,Williams', lang=['CSharp', 'VB'], state='NV')]\n",
      "James,,Smith,['Java', 'Scala', 'C++']\n",
      "Michael,Rose,,['Spark', 'Java', 'C++']\n",
      "Robert,,Williams,['CSharp', 'VB']\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- currentState: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "row=Row(\"James\",40)\n",
    "print(row[0] +\",\"+str(row[1]))\n",
    "row2=Row(name=\"Alice\", age=11)\n",
    "print(row2.name)\n",
    "\n",
    "Person = Row(\"name\", \"age\")\n",
    "p1=Person(\"James\", 40)\n",
    "p2=Person(\"Alice\", 35)\n",
    "print(p1.name +\",\"+p2.name)\n",
    "\n",
    "#PySpark Example\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "data = [Row(name=\"James,,Smith\",lang=[\"Java\",\"Scala\",\"C++\"],state=\"CA\"), \n",
    "    Row(name=\"Michael,Rose,\",lang=[\"Spark\",\"Java\",\"C++\"],state=\"NJ\"),\n",
    "    Row(name=\"Robert,,Williams\",lang=[\"CSharp\",\"VB\"],state=\"NV\")]\n",
    "\n",
    "#RDD Example 1\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "collData=rdd.collect()\n",
    "print(collData)\n",
    "for row in collData:\n",
    "    print(row.name + \",\" +str(row.lang))\n",
    "\n",
    "# RDD Example 2\n",
    "Person=Row(\"name\",\"lang\",\"state\")\n",
    "data = [Person(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \n",
    "    Person(\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"),\n",
    "    Person(\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "collData=rdd.collect()\n",
    "print(collData)\n",
    "for person in collData:\n",
    "    print(person.name + \",\" +str(person.lang))\n",
    "\n",
    "#DataFrame Example 1\n",
    "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
    "df=spark.createDataFrame(data)\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "collData=df.collect()\n",
    "print(collData)\n",
    "for row in collData:\n",
    "    print(row.name + \",\" +str(row.lang))\n",
    "    \n",
    "#DataFrame Example 2\n",
    "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
    "df=spark.createDataFrame(data).toDF(*columns)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aaa036-26f7-4a08-8983-002ed1224204",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
