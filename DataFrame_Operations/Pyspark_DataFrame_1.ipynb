{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4343454b-8d5c-438c-9b6f-5816691e5a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Dataframe from RDD\n",
    "\n",
    "# Import PySpark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Create SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"SparkByExamples.com\").getOrCreate()\n",
    "\n",
    "columns = [\"language\",\"users_count\"]\n",
    "data = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]\n",
    "\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "rdd = spark.sparkContext.parallelize(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b7b9a77-b5d2-405e-9781-3184bce4738c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#PySpark RDD’s toDF() method is used to create a DataFrame from the existing RDD. Since RDD doesn’t have columns, the DataFrame is created with\n",
    "#default column names\n",
    "\n",
    "dfFromRDD1 = rdd.toDF()\n",
    "dfFromRDD1.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6017648c-60d4-4036-a5b1-6811e456feef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- language: string (nullable = true)\n",
      " |-- users_count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "columns = [\"language\",\"users_count\"]\n",
    "dfFromRDD1 = rdd.toDF(columns)\n",
    "dfFromRDD1.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c9769d6-3ab5-4429-982a-66cc67419e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using createDataFrame() from SparkSession is another way to create manually and it takes rdd object as an argument.\n",
    "\n",
    "\n",
    "dfFromRDD2 = spark.createDataFrame(rdd).toDF(*columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8e6d986-c45f-4eb2-a3ca-528def4c6c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using createDataFrame() from SparkSession\n",
    "\n",
    "dfFromData2 = spark.createDataFrame(data).toDF(*columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "052ffc15-3850-4f02-8725-8310534c4716",
   "metadata": {},
   "outputs": [],
   "source": [
    "#createDataFrame() has another signature in PySpark which takes the collection of Row type and schema for column names as arguments.\n",
    "from pyspark.sql import Row\n",
    "\n",
    "rowData = map(lambda x: Row(*x), data) \n",
    "dfFromData3 = spark.createDataFrame(rowData,columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff388ba7-6b1d-40d5-b013-52c3a3d6b0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|id   |gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|James    |          |Smith   |36636|M     |3000  |\n",
      "|Michael  |Rose      |        |40288|M     |4000  |\n",
      "|Robert   |          |Williams|42114|M     |4000  |\n",
      "|Maria    |Anne      |Jones   |39192|F     |4000  |\n",
      "|Jen      |Mary      |Brown   |     |F     |-1    |\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create DataFrame with schema\n",
    "\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "data2 = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
    "  ]\n",
    "\n",
    "schema = StructType([ \\\n",
    "    StructField(\"firstname\",StringType(),True), \\\n",
    "    StructField(\"middlename\",StringType(),True), \\\n",
    "    StructField(\"lastname\",StringType(),True), \\\n",
    "    StructField(\"id\", StringType(), True), \\\n",
    "    StructField(\"gender\", StringType(), True), \\\n",
    "    StructField(\"salary\", IntegerType(), True) \\\n",
    "  ])\n",
    " \n",
    "df = spark.createDataFrame(data=data2,schema=schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5a21db-252d-4ac8-9c3a-6e2a877c070d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating DataFrame from CSV\n",
    "\n",
    "\n",
    "df2 = spark.read.csv(\"/src/resources/file.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50f342b-1529-4a93-9291-37d721acd001",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating from text (TXT) file\n",
    "\n",
    "df2 = spark.read.text(\"/src/resources/file.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9d5fb1-63a0-4738-8228-9b45f1630cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating from JSON file\n",
    "\n",
    "\n",
    "df2 = spark.read.json(\"/src/resources/file.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4a5f97-126e-4f74-b5b7-a3887d777037",
   "metadata": {},
   "source": [
    "Apache Parquet file is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model, or programming language.\n",
    "\n",
    "\n",
    "Advantages:\n",
    "While querying columnar storage, it skips the nonrelevant data very quickly, making faster query execution. As a result aggregation queries consume less time compared to row-oriented databases.\n",
    "\n",
    "It is able to support advanced nested data structures.\n",
    "\n",
    "Parquet supports efficient compression options and encoding schemes.\n",
    "\n",
    "Pyspark SQL provides support for both reading and writing Parquet files that automatically capture the schema of the original data, It also reduces data storage by 75% on average. Pyspark by default supports Parquet in its library hence we don’t need to add any dependency libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe95bd8-5766-40ee-954b-b7a7bd8a3807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PySpark Read and Write Parquet File\n",
    "\n",
    "df.write.parquet(\"/tmp/out/people.parquet\") \n",
    "parDF1=spark.read.parquet(\"/temp/out/people.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccaa6552-99c0-486e-b514-4c21755b2062",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apache Parquet Pyspark Example\n",
    "\n",
    "\n",
    "data =[(\"James \",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "              (\"Michael \",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "              (\"Robert \",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "              (\"Maria \",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "              (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)]\n",
    "columns=[\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "df=spark.createDataFrame(data,columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0ef74c6-7e1f-41da-b9ec-c4e86d9022da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pyspark Write DataFrame to Parquet file format\n",
    "\n",
    "df.write.parquet(\"/tmp/output/people.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b53ab47f-a1a8-48d2-ad4a-7cde5bd3e645",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pyspark Read Parquet file into DataFrame\n",
    "\n",
    "parDF=spark.read.parquet(\"/tmp/output/people.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ec416f2-e430-41d7-964a-a3eec1297a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append or Overwrite an existing Parquet file\n",
    "\n",
    "df.write.mode('append').parquet(\"/tmp/output/people.parquet\")\n",
    "df.write.mode('overwrite').parquet(\"/tmp/output/people.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8c13658-dd9e-4d30-8c1f-288302cbe284",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parqDF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Executing SQL queries DataFrame\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mparqDF\u001b[49m\u001b[38;5;241m.\u001b[39mcreateOrReplaceTempView(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParquetTable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m parkSQL \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselect * from ParquetTable where salary >= 4000 \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parqDF' is not defined"
     ]
    }
   ],
   "source": [
    "#Executing SQL queries DataFrame\n",
    "\n",
    "parqDF.createOrReplaceTempView(\"ParquetTable\")\n",
    "parkSQL = spark.sql(\"select * from ParquetTable where salary >= 4000 \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e75e2b-2aac-48ed-b424-6e11f5044472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
